{"name":"Mypipe","tagline":"MySQL binary log consumer with the ability to act on changed rows and publish changes to different systems with emphasis on Apache Kafka.","body":"# mypipe\r\nmypipe latches onto a MySQL server with binary log replication enabled and\r\nallows for the creation of pipes that can consume the replication stream and\r\nact on the data (primarily integrated with Apache Kafka).\r\n\r\n# Features\r\n* streams binary logs remotely, emulating a slave\r\n* writes binlog events into Kafka using a generic or specific Avro schema\r\n* supports saving / loading binary log positions in a modular fashion (files, MySQL, or custom Java/Scala code to do so) \r\n* handles `ALTER TABLE` events and can refresh Avro schema being used\r\n* built in a modular way allowing binlog events to be published into any system, not just Kafka\r\n* can preload an entire MySQL table into Kafka, then resume from binary logs (useful with Kafka compaction, infinite retention, can be used to bootstrap downstream systems with the entire data for a table)\r\n* configurable Kafka topic names based on the database and table\r\n* whitelist / blacklist support for what to process or what not to process in a binary log (based on database and table)\r\n* configurable error handling with the ability to specify custom handlers written in Java or Scala\r\n* Kafka generic console consumer that interfaces with an in memory Avro schema repo for quick and easy data exploration in Kafka\r\n\r\n# API\r\nmypipe tries to provide enough information that usually is not part of the\r\nMySQL binary log stream so that the data is meaningful. mypipe requires a\r\nrow based binary log format and provides `Insert`, `Update`, and `Delete`\r\nmutations representing changed rows. Each change is related back to it's\r\ntable and the API provides metadata like column types, primary key\r\ninformation (composite, key order), and other such useful information.\r\n\r\nLook at `ColumnType.scala` and `Mutation.scala` for more details.\r\n\r\n# Producers\r\nProducers receive MySQL binary log events and act on them. They can funnel\r\ndown to another data store, send them to some service, or just print them\r\nout to the screen in the case of the stdout producer.\r\n\r\n# Pipes\r\nPipes tie one or more MySQL binary log consumers to a producer. Pipes can be\r\nused to create a system of fan-in data flow from several MySQL servers to\r\nother data sources such as Hadoop, Cassandra, Kafka, or other MySQL servers.\r\nThey can also be used to update or flush caches.\r\n\r\n# Kafka integration\r\nmypipe's main goal is to replicate a MySQL binlog stream into Apache Kafka.\r\nmypipe supports Avro encoding and can use a schema repository to figure out\r\nhow to encode data. Data can either be encoded generically and stored in\r\nAvro maps by type (integers, strings, longs, etc.) or it can encode data\r\nmore specifically if the schema repository can return specific schemas. The\r\nlatter will allow the table's structure to be reflected in the Avro structure.\r\n\r\n## Kafka message format\r\nBinary log events, specifically mutation events (insert, update, delete) are\r\npushed into Kafka and are binary encoded. Every message has the following \r\nformat:\r\n\r\n     -----------------\r\n    | MAGIC | 1 byte  |\r\n    |-----------------|\r\n    | MTYPE | 1 byte  |\r\n    |-----------------|\r\n    | SCMID | N bytes |\r\n    |-----------------|\r\n    | DATA  | N bytes |\r\n     -----------------\r\n\r\nThe above fields are:\r\n\r\n* `MAGIC`: magic byte, used to figure out protocol version\r\n* `MTYPE`: mutation type, a single byte indicating insert (`0x1`), update (`0x2`), or delete (`0x3`)\r\n* `SCMID`: Avro schema ID, variable number of bytes\r\n* `DATA`: the actual mutation data as bytes, variable size\r\n\r\n## MySQL to \"generic\" Kafka topics\r\nIf you do not have an Avro schema repository running that contains schemas \r\nfor each of your tables you can use generic Avro encoding. This will take \r\nbinary log mutations (insert, update, or delete) and encode them into the \r\nfollowing structure in the case of an insert (`InsertMutation.avsc`):\r\n\r\n    {\r\n      \"namespace\": \"mypipe.avro\",\r\n      \"type\": \"record\",\r\n      \"name\": \"InsertMutation\",\r\n      \"fields\": [\r\n    \t    {\r\n    \t\t\t\"name\": \"database\",\r\n    \t\t\t\"type\": \"string\"\r\n    \t\t},\r\n    \t\t{\r\n    \t\t\t\"name\": \"table\",\r\n    \t\t\t\"type\": \"string\"\r\n    \t\t},\r\n    \t\t{\r\n    \t\t\t\"name\": \"tableId\",\r\n    \t\t\t\"type\": \"long\"\r\n    \t\t},\r\n    \t\t{\r\n    \t\t\t\"name\": \"integers\",\r\n    \t\t\t\"type\": {\"type\": \"map\", \"values\": \"int\"}\r\n    \t\t},\r\n    \t\t{\r\n    \t\t\t\"name\": \"strings\",\r\n    \t\t\t\"type\": {\"type\": \"map\", \"values\": \"string\"}\r\n    \t\t},\r\n    \t\t{\r\n    \t\t\t\"name\": \"longs\",\r\n    \t\t\t\"type\": {\"type\": \"map\", \"values\": \"long\"}\r\n    \t\t}\r\n    \t]\r\n    }\r\n\r\nUpdates will contain both the old row values and the new ones (see \r\n`UpdateMutation.avsc`) and deletes are similar to inserts (`DeleteMutation.avsc`). \r\nOnce transformed into Avro data the mutations are pushed into Kafka topics \r\nbased on the following convention (this is configurable):\r\n\r\n    topicName = s\"$db_$table_generic\"\r\n\r\nThis ensures that all mutations destined to a specific database / table tuple are\r\nall added to a single topic with mutation ordering guarantees.\r\n\r\n## MySQL to \"specific\" Kafka topics\r\nIf you are running an Avro schema repository you can encode binary log events based on \r\nthe structures specified in that repository for the incoming database / table streams. \r\n\r\nIn order to configure a specific producer you should add a `pipe` using a `mypipe.producer.KafkaMutationSpecificAvroProducer` \r\nas it's producer. The producer needs some configuration values in order to find the Kafka brokers, \r\nZooKeeper ensemble, and the Avro schema repository. Here is a sample configuration:\r\n\r\n    kafka-specific {\r\n\t  enabled = true\r\n\t  consumers = [\"localhost\"]\r\n\t  producer {\r\n\t    kafka-specific {\r\n\t      schema-repo-client = \"mypipe.avro.schema.SchemaRepo\"\r\n          metadata-brokers = \"localhost:9092\"\r\n          zk-connect = \"localhost:2181\"\r\n\t    }\r\n\t  }\r\n\t}\r\n\r\nNote that if you use `mypipe.avro.schema.SchemaRepo` as the schema repository client, you have to \r\nprovide the running JVM with the system property `avro.repo.server-url` in order for the client to \r\nknow where to reach the repository.\r\n\r\n## Configuring Kafka topic names\r\nmypipe will push events into Kafka based on the `topic-format` configuration value. `reference.conf` \r\nhas the default values under `mypipe.kafka`, `specific-producer` and `generic-producer`.\r\n\r\n    specific-producer {\r\n      topic-format = \"${db}_${table}_specific\"\r\n    }\r\n     \r\n    generic-producer {\r\n      topic-format = \"${db}_${table}_generic\"\r\n    }\r\n\r\n## `ALTER` queries and \"generic\" Kafka topics\r\nmypipe handles `ALTER` table queries (as described below) allowing it to add \r\nnew columns or stop including removed ones into \"generic\" Avro records published \r\ninto Kafka. Since the \"generic\" Avro beans consist of typed maps (ints, strings, etc.) \r\nmypipe can easily include or remove columns based on `ALTER` queries. Once a table's \r\nmetadata is refreshed (blocking operation) all subsequent mutations to the \r\ntable will use the new structure and publish that into Kafka.\r\n\r\n## Consuming from \"generic\" Kafka topics\r\nIn order to consume from generic Kafka topics the `KafkaGenericMutationAvroConsumer` \r\ncan be used. This consumer will allow you react to insert, update, and delete mutations. \r\nThe consumer needs an Avro schema repository as well as some helpers to be defined. A quick \r\n(and incomplete) example follows:\r\n\r\n    val kafkaConsumer = new KafkaGenericMutationAvroConsumer[Short](\r\n      topic = KafkaUtil.genericTopic(\"databaseName\", \"tableName\"),\r\n      zkConnect = \"localhost:2181\",\r\n      groupId = \"someGroupId\",\r\n      schemaIdSizeInBytes = 2)(\r\n      \r\n      insertCallback = { insertMutation ⇒ ??? }\r\n      updateCallback = { updateMutation ⇒ ??? }\r\n      deleteCallback = { deleteMutation ⇒ ??? } \r\n    ) {\r\n      protected val schemaRepoClient: GenericSchemaRepository[Short, Schema] = GenericInMemorySchemaRepo\r\n    }\r\n\r\nFor a more complete example take a look at `KafkaGenericSpec.scala`.\r\n\r\nAlternatively you can implement your own Kafka consumer given the binary structure \r\nof the messages as shown above if the `KafkaGenericMutationAvroConsumer` does not \r\nsatisfy your needs.\r\n\r\n# MySQL Event Processing Internals\r\nmypipe uses the [mysql-binlog-connector-java](https://github.com/shyiko/mysql-binlog-connector-java) to tap \r\ninto the MySQL server's binary log stream and handles several types of events.\r\n\r\n## `TABLE_MAP`\r\nThis event causes mypipe to look up a table's metadata (primary key, column names and types, etc.).\r\nThis is done by issuing the following query to the MySQL server to determine column information:\r\n\r\n    select COLUMN_NAME, DATA_TYPE, COLUMN_KEY \r\n    from COLUMNS \r\n    where TABLE_SCHEMA=\"$db\" and TABLE_NAME = \"$table\" \r\n    order by ORDINAL_POSITION\r\n\r\nThe following query is issued to the server also to determine the primary key:\r\n\r\n    select COLUMN_NAME\r\n    from KEY_COLUMN_USAGE \r\n    where TABLE_SCHEMA='${db}' and TABLE_NAME='${table}' and CONSTRAINT_NAME='PRIMARY' \r\n    order by ORDINAL_POSITION\r\n\r\nWhile a `TABLE_MAP` event is being handled no other events will be handled concurrently.\r\n\r\n## `QUERY`\r\nmypipe handles a few different types of raw queries (besides mutations) like:\r\n\r\n### `BEGIN`\r\nIf transaction event grouping is enabled mypipe will queue up all events that\r\narrive after a `BEGIN` query has been encountered. While queuing is occuring \r\nmypipe will not save it's binary log position as it receives events and will \r\nonly do so once the transaction is committed.\r\n\r\n### `COMMIT`\r\nIf transaction event grouping is enabled mypipe will hold wait for a `COMMIT` \r\nquery to arrive before \"flushing\" all queued events (mutations) at and then \r\nsaves it's binary log position to mark the processing of the entire transaction.\r\n\r\n### `ROLLBACK`\r\nIf transaction event grouping is enabled mypipe will clear and not flush the \r\nqueued up events (mutations) upon receiving a `ROLBACK`.\r\n\r\n### `ALTER`\r\nUpon receiving an `ALTER` query mypipe will attempt to reload the affected table's \r\nmetadata. This allows mypipe to detect dropped / added columns, changed keys, or \r\nanything else that could affect the table. mypipe will perform a look up similar \r\nto the one done when handling a `TABLE_MAP` event.\r\n\r\n# Getting Started\r\nThis section aims to guide you through setting up MySQL so that it can be used\r\nwith mypipe. It then goes into setting up mypipe itself to push binary log events\r\ninto Kafka. Finally, it explains how to consume these events from Kafka.\r\n\r\n## Enabling MySQL binary logging\r\n\r\nThe following snippet of configuration will enable MySQL to generate binary logs \r\nthe mypipe will be able to understand. Taken from `my.cnf`:\r\n\r\n    server-id         = 112233\r\n    log_bin           = mysql-bin\r\n    expire_logs_days  = 1\r\n    binlog_format     = row\r\n\r\nThe binary log format is required to be set to `row` for mypipe to work since \r\nwe need to track individual changes to rows (insert, update, delete).\r\n\r\n## Configuring mypipe to ingest MySQL binary logs\r\n\r\nCurrently, mypipe does not offset a binary distribution that can be installed.\r\nIn order use mypipe, start by cloning the git repository:\r\n\r\n    git clone https://github.com/mardambey/mypipe.git\r\n\r\nCompile and package the code:\r\n\r\n    ./sbt package\r\n\r\nmypipe needs some configuration in order to point it to a MySQL server.\r\nThe following configuration goes into `application.conf` in the `mypipe-runner`\r\nsub-project. Some other configuration entries can be added to other files\r\nas well; when indicated.\r\n\r\nA MySQL server can be defined in the configuration under the `consumers` \r\nsection. For example:\r\n\r\n    # consumers represent sources for mysql binary logs\r\n    consumers {\r\n  \r\n    database1  {\r\n        # database \"host:port:user:pass\" \r\n        source = \"localhost:3306:mypipe:mypipe\"\r\n      }\r\n    }\r\n\r\nMultiple consumers (or sources of data) can be defined. Once mypipe\r\nlatches onto a MySQL server, it will attempt to pick up where it last\r\nleft of if an offset was previously saved for the given database host\r\nand database name combination. mypipe saves it's offsets in files for\r\nnow. These files are stored in the location indicated by the config\r\nentry `data-dir` in the `mypipe-api` project's `reference.conf`.\r\n\r\n    data-dir = \"/tmp/mypipe\"\r\n\r\nYou can either override that config in `application.conf` of `mypipe-runner`\r\nor edit it in `mypipe-api` directly.\r\n\r\nThe simplest way to observe the replication stream that mypipe is \r\ningesting is to configure the `stdout` producer. This producer will\r\nsimply print to standard out the mutation events that mypipe is \r\nconsuming.\r\n\r\nThe following snippet goes into the configuration file in order to\r\ndo so:\r\n\r\n    producers {\r\n      stdout {\r\n         class = \"mypipe.producer.stdout.StdoutProducer\"\r\n      }\r\n    }\r\n\r\nIn order to instruct mypipe to connect the consumer called `database1`\r\nand the `stdout` producer they must be joined by a `pipe`.\r\n\r\nThe following configuration entry creates such a pipe:\r\n\r\n    pipes {\r\n  \r\n      # prints queries to standard out\r\n      stdout {\r\n        consumers = [\"database1\"]\r\n        producer {\r\n          stdout {}\r\n        }\r\n      }\r\n    }\r\n\r\nAt this point, this configuration can be tested out. The\r\n`mypipe-runner` project can run mypipe and use these configuration\r\nentries.\r\n\r\n    ./sbt \"project runner\" \"runMain mypipe.runner.PipeRunner\"\r\n\r\nAs soon as mypipe starts, it will latch onto the binary log\r\nstream and the `stdout` producer will print out mutations to the\r\nconsole.\r\n\r\n## Configuring mypipe to produce events into Kafka\r\nmypipe can produce mutations into Kafka either generically or more\r\nspecifically. See the above sections for more explanation on what\r\nthis means.\r\n\r\n### Generic Kafka topics\r\nIn order to set up mypipe to produce events to Kafka generically, a\r\nproducer must be added to the `producers` section of the configuration.\r\nThis is similar to the `stdout` producer added earlier.\r\n\r\n    producers {\r\n        kafka-generic {\r\n            class = \"mypipe.producer.KafkaMutationGenericAvroProducer\"\r\n        }\r\n    }\r\n\r\nThis producer must then be joined with a data consumer using a pipe.\r\nThe configuration of the Kafka brokers will also be passed to the\r\nproducer.\r\n\r\n    pipes {\r\n        kafka-generic {\r\n            enabled = true\r\n            consumers = [\"database1\"]\r\n            producer {\r\n              kafka-generic {\r\n                 metadata-brokers = \"localhost:9092\"\r\n              }\r\n            }\r\n        }\r\n    }\r\n\r\nThis pipe connects `database1` with the Kafka cluster defined by\r\nthe values in the `meta-brokers` key, brokers must be comma separated.\r\nAs mentioned before, once this pipe is active it will produce events\r\ninto Kafka topics based on the database and table the mutations are\r\ncoming from. Topics are named as such:\r\n\r\n    $database_$table_generic\r\n\r\nEach of these topics contain ordered mutations for the database / table\r\ntuple.\r\n\r\nFor a more complete configuration file, take a look at the sample\r\nconfiguration shown later in this document.\r\n\r\n### Consuming mutations pushed into Kafka via console (generically)\r\nIn order to generically consume and display the mutations pushed into\r\nKafka, the provided generic console consumer will help. It can be\r\nused by invoking the following command:\r\n\r\n    ./sbt \"project runner\" \\\r\n    \"runMain mypipe.runner.KafkaGenericConsoleConsumer $database_$table_generic zkHost:2181 groupId\"\r\n\r\nThis will consume messages for the given `$database` / `$table` tuple from\r\nthe Kafka cluster specified by the given ZooKeeper ensemble connection\r\nstring and the consumer group ID to use.\r\n\r\n# Error Handling\r\nIf the default configuration based error handler is used:\r\n\r\n    mypipe.error-handler { default { class = \"mypipe.mysql.ConfigBasedErrorHandler\" } }\r\n\r\nthen user can decide to abort and stop processing using the following flags:\r\n\r\n* `quit-on-event-handler-failure`: an event handler failed (commit)\r\n* `quit-on-event-decode-failure`: mypipe could not decode the event\r\n* `quit-on-listener-failure`: a specified listener could not process the event\r\n* `quit-on-empty-mutation-commit-failure`: quit upon encountering an empty transaction\r\n\r\nErrors are handled as such:\r\n\r\n* The first handler deals with event decoding errors (ie: mypipe can not determine the event type and decode it).\r\n* The second layer of error handlers deals with specific event errors, for example: mutation, alter, table map, commit.\r\n* The third and final layer is the global error handler.\r\n\r\nError handler invocation:\r\n\r\n* If the first layer or second layer are invoked and they return true, the next event will be consumed and the global error handler is not called.\r\n* If the first layer or second layer are invoked and they return false, then the third layer (global error handler) is invoked, otherwise, processing of the next event continues\r\n\r\nA custom error handler can also be provided if available in the classpath.\r\n\r\n# Event Filtering\r\nIf not all events are to be processed, they can be filtered by setting `include-event-condition`.\r\nThis allows for controlling what dbs and tables will be consumed. Effectively, this is treated as Scala code and is compiled at runtime. Setting this value to blank ignores it.\r\nExample:\r\n\r\n    include-event-condition = \"\"\" db == \"mypipe\" && table ==\"user\" \"\"\"\r\n\r\nThe above will only process events originating from the database named \"mypipe\" and the table named \"user\".\r\n\r\n# Tests\r\nIn order to run the tests you need to configure `test.conf` with proper MySQL\r\nvalues. You should also make sure there you have a database called `mypipe` with\r\nthe following credentials:\r\n\r\n    GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'mypipe'@'%' IDENTIFIED BY 'mypipe'\r\n    GRANT ALL PRIVILEGES ON `mypipe`.* TO 'mypipe'@'%'\r\n\r\nThe database must also have binary logging enabled in `row` format.\r\n\r\n# Sample application.conf\r\n\r\n    mypipe {\r\n    \r\n      # Avro schema repository client class name\r\n      schema-repo-client = \"mypipe.avro.schema.SchemaRepo\"\r\n    \r\n      # consumers represent sources for mysql binary logs\r\n      consumers {\r\n    \r\n        localhost {\r\n          # database \"host:port:user:pass\" array\r\n          source = \"localhost:3306:mypipe:mypipe\"\r\n        }\r\n      }\r\n    \r\n      # data producers export data out (stdout, other stores, external services, etc.)\r\n      producers {\r\n    \r\n        stdout {\r\n          class = \"mypipe.producer.stdout.StdoutProducer\"\r\n        }\r\n    \r\n        kafka-generic {\r\n          class = \"mypipe.producer.KafkaMutationGenericAvroProducer\"\r\n        }\r\n      }\r\n    \r\n      # pipes join consumers and producers\r\n      pipes {\r\n    \r\n        stdout {\r\n          consumers = [\"localhost\"]\r\n          producer {\r\n            stdout {}\r\n          }\r\n          # how to save and load binary log positions\r\n          binlog-position-repo {\r\n            # saved to a file, this is the default if unspecified\r\n            class = \"mypipe.api.repo.ConfigurableFileBasedBinaryLogPositionRepository\"\r\n            config {\r\n              file-prefix = \"stdout-00\"     # required if binlog-position-repo is specifiec\r\n              data-dir = \"/tmp/mypipe/data\" # defaults to mypipe.data-dir if not present\r\n            }\r\n          }\r\n        }\r\n    \r\n        kafka-generic {\r\n          enabled = true\r\n          consumers = [\"localhost\"]\r\n          producer {\r\n            kafka-generic {\r\n              metadata-brokers = \"localhost:9092\"\r\n            }\r\n          }\r\n          binlog-position-repo {\r\n            # saves to a MySQL database, make sure you use the following as well to prevent reacting on\r\n            # inserts / updates made in the same DB being listenened on for changes\r\n            # mypipe {\r\n            #   include-event-condition = \"\"\" table != \"binlogpos\" \"\"\"\r\n            #   error {\r\n            #     quit-on-empty-mutation-commit-failure = false\r\n            #   }\r\n            # }\r\n            class = \"mypipe.api.repo.ConfigurableMySQLBasedBinaryLogPositionRepository\"\r\n            config {\r\n              # database \"host:port:user:pass\" array\r\n              source = \"localhost:3306:mypipe:mypipe\"\r\n              database = \"mypipe\"\r\n              table = \"binlogpos\"\r\n              id = \"kafka-generic\" # used to find the row in the table for this pipe\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\n# mailing list\r\nhttps://groups.google.com/d/forum/mypipe\r\n","google":"UA-71411370-1","note":"Don't delete this file! It's used internally to help with page regeneration."}